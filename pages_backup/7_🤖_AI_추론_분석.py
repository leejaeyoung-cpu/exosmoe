"""
ğŸ¤– AI ì¶”ë¡  ë¶„ì„ í˜ì´ì§€
Cellpose + Deep Learning ê¸°ë°˜ MT-EXO ê¸°ëŠ¥ ìë™ ë¶„ë¥˜
"""

import streamlit as st
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import cv2
from PIL import Image
import plotly.express as px
import plotly.graph_objects as go

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.utils import set_page_config, sidebar_info

# í˜ì´ì§€ ì„¤ì •
set_page_config("AI ì¶”ë¡  ë¶„ì„")

st.title("ğŸ¤– AI ì¶”ë¡  ë¶„ì„")
st.markdown("### Cellpose + Deep Learning ê¸°ë°˜ ì„¸í¬ ê¸°ëŠ¥ ìë™ ë¶„ë¥˜")

# Session state ì´ˆê¸°í™”
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = []
if 'show_results' not in st.session_state:
    st.session_state.show_results = False

# ì •ë³´ ë°•ìŠ¤
with st.expander("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´", expanded=False):
    st.markdown("""
    ### ğŸ”¬ ë¶„ì„ íŒŒì´í”„ë¼ì¸
    
    1. **Cellpose ì„¸ê·¸ë©˜í…Œì´ì…˜**
       - ì„¸í¬ ìë™ ê°ì§€ ë° ë¶„ë¦¬
       - 20ì°¨ì› íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
    
    2. **ë”¥ëŸ¬ë‹ ë¶„ë¥˜**
       - ResNet50 + Attention Mechanism
       - 5ê°œ ê¸°ëŠ¥ ë¶„ë¥˜ (í•­ì‚°í™”, í•­ì„¬ìœ í™”, í•­ì—¼ì¦, í˜ˆê´€í˜•ì„±, ì„¸í¬ì¦ì‹)
    
    3. **ì„¤ëª… ê°€ëŠ¥í•œ AI**
       - Grad-CAMìœ¼ë¡œ ì¤‘ìš” ì˜ì—­ ì‹œê°í™”
       - ì‹ ë¢°ë„ ì ìˆ˜ ì œê³µ
    """)

# íƒ­ êµ¬ì„±
tab1, tab2, tab3 = st.tabs(["ğŸ“¤ ì´ë¯¸ì§€ ì—…ë¡œë“œ", "ğŸ”¬ HUVEC ë°ì´í„° ë¶„ì„", "ğŸ“Š ë¶„ì„ ê²°ê³¼"])

# === íƒ­ 1: ì´ë¯¸ì§€ ì—…ë¡œë“œ ===
with tab1:
    st.header("ì´ë¯¸ì§€ ì—…ë¡œë“œ ë° ë¶„ì„")
    
    uploaded_files = st.file_uploader(
        "ì„¸í¬ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=['jpg', 'jpeg', 'png'],
        accept_multiple_files=True
    )
    
    col1, col2 = st.columns(2)
    with col1:
        run_cellpose = st.checkbox("Cellpose ì„¸ê·¸ë©˜í…Œì´ì…˜", value=True)
    with col2:
        run_gradcam = st.checkbox("Grad-CAM ì„¤ëª…", value=False)
    
    if uploaded_files:
        st.info(f"ğŸ“ {len(uploaded_files)}ê°œ ì´ë¯¸ì§€ ì—…ë¡œë“œë¨")
        
        if st.button("ğŸš€ AI ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
            
            # ëª¨ë¸ ë¡œë”©
            with st.spinner("AI ì—”ì§„ ì´ˆê¸°í™” ì¤‘..."):
                try:
                    from src.mt_exo_inference import MTEXOInferenceEngine
                    engine = MTEXOInferenceEngine(use_gpu=True)
                    st.success("âœ… AI ì—”ì§„ ë¡œë“œ ì™„ë£Œ!")
                except Exception as e:
                    st.error(f"âŒ ì—”ì§„ ë¡œë“œ ì‹¤íŒ¨: {e}")
                    st.stop()
            
            # ì„ì‹œ ì €ì¥
            temp_dir = project_root / "data" / "temp_ai_inference"
            temp_dir.mkdir(exist_ok=True, parents=True)
            
            image_paths = []
            for uploaded_file in uploaded_files:
                file_path = temp_dir / uploaded_file.name
                with open(file_path, 'wb') as f:
                    f.write(uploaded_file.getbuffer())
                image_paths.append(str(file_path))
            
            # ë¶„ì„ ì‹¤í–‰
            with st.spinner(f"AI ì¶”ë¡  ì¤‘... ({len(image_paths)}ê°œ ì´ë¯¸ì§€)"):
                try:
                    results = engine.batch_predict(image_paths, explain=run_gradcam)
                    
                    # ì„¸ì…˜ì— ì €ì¥
                    st.session_state.analysis_results = results
                    st.session_state.show_results = True
                    
                    st.success(f"âœ… {len(results)}ê°œ ì´ë¯¸ì§€ ë¶„ì„ ì™„ë£Œ!")
                    
                    # í˜ì´ì§€ ì¬ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ í‘œì‹œ
                    st.rerun()
                    
                except Exception as e:
                    st.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    import traceback
                    st.code(traceback.format_exc())
                    st.stop()
    
    # ê²°ê³¼ í‘œì‹œ
    if st.session_state.show_results and len(st.session_state.analysis_results) > 0:
        st.markdown("---")
        st.subheader("ğŸ“Š ë¶„ì„ ê²°ê³¼")
        
        for i, result in enumerate(st.session_state.analysis_results):
            if 'prediction' not in result:
                continue
            
            with st.container():
                st.markdown(f"### ğŸ“· ì´ë¯¸ì§€ #{i+1}")
                
                col1, col2, col3 = st.columns([2, 2, 3])
                
                with col1:
                    # ì›ë³¸ ì´ë¯¸ì§€
                    try:
                        img = Image.open(result['image_path'])
                        st.image(img, caption="ì›ë³¸ ì´ë¯¸ì§€", use_container_width=True)
                    except:
                        st.warning("ì´ë¯¸ì§€ í‘œì‹œ ì‹¤íŒ¨")
                
                with col2:
                    # ì˜ˆì¸¡ ê²°ê³¼
                    st.metric("ğŸ¯ ì˜ˆì¸¡ ê¸°ëŠ¥", result['prediction']['class_name'])
                    st.metric("ğŸ“Š ì‹ ë¢°ë„", f"{result['prediction']['confidence']:.1%}")
                    
                    if 'cellpose' in result:
                        st.metric("ğŸ”¬ ê²€ì¶œ ì„¸í¬", result['cellpose']['num_cells'])
                
                with col3:
                    # í™•ë¥  ë¶„í¬
                    probs = result['prediction']['probabilities']
                    prob_df = pd.DataFrame({
                        'ê¸°ëŠ¥': list(probs.keys()),
                        'í™•ë¥ ': list(probs.values())
                    })
                    
                    fig = px.bar(
                        prob_df, 
                        x='ê¸°ëŠ¥', 
                        y='í™•ë¥ ',
                        title='ê¸°ëŠ¥ë³„ í™•ë¥  ë¶„í¬',
                        color='í™•ë¥ ',
                        color_continuous_scale='Viridis',
                        range_y=[0, 1]
                    )
                    fig.update_layout(height=300)
                    st.plotly_chart(fig, use_container_width=True, key=f"prob_chart_{i}")
                
                st.divider()
        
        # ì´ˆê¸°í™” ë²„íŠ¼
        if st.button("ğŸ”„ ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘"):
            st.session_state.analysis_results = []
            st.session_state.show_results = False
            st.rerun()

# === íƒ­ 2: HUVEC ë°ì´í„° ë¶„ì„ ===
with tab2:
    st.header("HUVEC TNF-Î± ë°ì´í„° ìë™ ë¶„ì„")
    
    st.info("ğŸ’¡ ê¸°ì¡´ HUVEC ë°ì´í„°ë¥¼ AI ëª¨ë¸ë¡œ ìë™ ë¶„ë¥˜í•©ë‹ˆë‹¤.")
    
    huvec_dir = project_root / "data" / "HUVEC TNF-a" / "HUVEC TNF-a" / "251209"
    
    if huvec_dir.exists():
        image_files = list(huvec_dir.glob("*.jpg"))
        st.write(f"ğŸ“ ë°œê²¬ëœ ì´ë¯¸ì§€: {len(image_files)}ê°œ")
        
        num_analyze = st.slider("ë¶„ì„í•  ì´ë¯¸ì§€ ìˆ˜", 1, min(len(image_files), 20), 6)
        
        if st.button("ğŸ”¬ ìë™ ë¶„ì„ ì‹œì‘", type="primary"):
            with st.spinner("AI ì—”ì§„ ë¡œë”©..."):
                try:
                    from src.mt_exo_inference import MTEXOInferenceEngine
                    engine = MTEXOInferenceEngine(use_gpu=True)
                    
                    selected_images = [str(f) for f in image_files[:num_analyze]]
                    
                    with st.spinner(f"{num_analyze}ê°œ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘..."):
                        results = engine.batch_predict(selected_images, explain=False)
                    
                    # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„
                    df_results = pd.DataFrame([
                        {
                            'ì´ë¯¸ì§€': Path(r['image_path']).name,
                            'ì˜ˆì¸¡ ê¸°ëŠ¥': r['prediction']['class_name'],
                            'ì‹ ë¢°ë„': r['prediction']['confidence'],
                            'ì„¸í¬ ìˆ˜': r['cellpose']['num_cells']
                        }
                        for r in results if 'prediction' in r
                    ])
                    
                    st.success(f"âœ… {len(df_results)}ê°œ ë¶„ì„ ì™„ë£Œ!")
                    
                    # ìš”ì•½
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("í‰ê·  ì‹ ë¢°ë„", f"{df_results['ì‹ ë¢°ë„'].mean():.1%}")
                    with col2:
                        st.metric("í‰ê·  ì„¸í¬ ìˆ˜", f"{df_results['ì„¸í¬ ìˆ˜'].mean():.0f}")
                    with col3:
                        most_common = df_results['ì˜ˆì¸¡ ê¸°ëŠ¥'].mode()[0]
                        st.metric("ì£¼ìš” ê¸°ëŠ¥", most_common)
                    
                    # í…Œì´ë¸”
                    st.dataframe(df_results, use_container_width=True, height=400)
                    
                    # ë¶„í¬ ì°¨íŠ¸
                    fig = px.histogram(
                        df_results, 
                        x='ì˜ˆì¸¡ ê¸°ëŠ¥',
                        title='ê¸°ëŠ¥ ë¶„ë¥˜ ë¶„í¬',
                        color='ì˜ˆì¸¡ ê¸°ëŠ¥'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                except Exception as e:
                    st.error(f"âŒ ì˜¤ë¥˜: {e}")
                    import traceback
                    st.code(traceback.format_exc())
    else:
        st.warning(f"âš ï¸ HUVEC ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {huvec_dir}")

# === íƒ­ 3: ë¶„ì„ ê²°ê³¼ ===
with tab3:
    st.header("ì €ì¥ëœ ë¶„ì„ ê²°ê³¼")
    
    results_path = project_root / "data" / "AI_Inference_Results" / "inference_results.json"
    
    if results_path.exists():
        import json
        with open(results_path, 'r', encoding='utf-8') as f:
            saved_results = json.load(f)
        
        st.success(f"âœ… {len(saved_results)}ê°œ ë¶„ì„ ê¸°ë¡ ë°œê²¬")
        
        # ê²°ê³¼ í‘œì‹œ
        for i, result in enumerate(saved_results, 1):
            if 'prediction' in result:
                with st.expander(f"#{i} - {Path(result['image_path']).name}"):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**ì˜ˆì¸¡ ì •ë³´**")
                        st.write(f"- ê¸°ëŠ¥: {result['prediction']['class_name']}")
                        st.write(f"- ì‹ ë¢°ë„: {result['prediction']['confidence']:.3f}")
                        st.write(f"- ë¶„ì„ ì‹œê°: {result['timestamp']}")
                    
                    with col2:
                        st.write("**Cellpose ì •ë³´**")
                        st.write(f"- ì„¸í¬ ìˆ˜: {result['cellpose']['num_cells']}")
                        st.write(f"- íŠ¹ì§• ì°¨ì›: {len(result['cellpose']['feature_vector'])}")
    else:
        st.info("ğŸ’¡ ì•„ì§ ì €ì¥ëœ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°”
sidebar_info()

# ëª¨ë¸ ìƒíƒœ
with st.sidebar:
    st.header("ğŸ¤– ëª¨ë¸ ìƒíƒœ")
    
    try:
        import torch
        gpu_available = torch.cuda.is_available()
        
        if gpu_available:
            st.success("âœ… GPU ì‚¬ìš© ê°€ëŠ¥")
            st.write(f"GPU: {torch.cuda.get_device_name(0)}")
        else:
            st.warning("âš ï¸ CPU ëª¨ë“œ")
        
        st.write(f"PyTorch: {torch.__version__}")
        
    except:
        st.error("âŒ PyTorch ë¯¸ì„¤ì¹˜")
    
    st.divider()
    
    st.markdown("""
    ### ğŸ’¡ ì‚¬ìš© íŒ
    
    - GPU ì‚¬ìš© ì‹œ í›¨ì”¬ ë¹ ë¦„
    - ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ ì´ë¯¸ì§€ ë™ì‹œ ë¶„ì„
    - Grad-CAMìœ¼ë¡œ AI íŒë‹¨ ê·¼ê±° í™•ì¸
    """)
